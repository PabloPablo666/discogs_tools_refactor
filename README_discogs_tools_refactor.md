# Discogs Data Lake Â· Pipelines & Tests

This repository contains **production-grade ingestion pipelines**, validation tests,
and SQL utilities used to build a **typed, reproducible Discogs data lake** stored as
**Parquet** and queried with **DuckDB** and **Trino**.

The focus is **correctness, reproducibility, and schema stability**, not quick hacks.

This repository is the **pipeline and validation layer** of the Discogs lakehouse.
Infrastructure and query serving live in a separate repository.

---

## ğŸ“¦ What this repo is

A **refactored, testable pipeline layer** for ingesting Discogs dumps into a
**typed lakehouse layout**:

- Streaming XML â†’ Parquet
- Canonical, typed schemas
- Deterministic outputs
- Explicit sanity tests
- Legacy scripts preserved for auditability

This repo does **not** ship data.  
It ships **code that produces data deterministically**.

---

## ğŸ—‚ Repository structure

discogs_tools_refactor/
â”œâ”€â”€ pipelines/ # Maintained ingestion pipelines (typed outputs)
â”‚ â”œâ”€â”€ extract_artists_v1.py
â”‚ â”œâ”€â”€ extract_artist_relations_v1.py
â”‚ â”œâ”€â”€ extract_masters_v1.py
â”‚ â”œâ”€â”€ extract_releases_v6.py
â”‚ â”œâ”€â”€ parse_labels_v10.py
â”‚ â””â”€â”€ rebuild_artist_name_map_v1.py
â”‚
â”œâ”€â”€ tests/ # Runnable sanity tests (DuckDB-based)
â”‚ â”œâ”€â”€ run_test_artists_v1.sh
â”‚ â”œâ”€â”€ run_test_artist_relations.sh
â”‚ â”œâ”€â”€ run_test_masters_v1.sh
â”‚ â”œâ”€â”€ run_test_labels_v10.sh
â”‚ â””â”€â”€ run_test_releases_v6.sh
â”‚
â”œâ”€â”€ sql/ # Analytical & sanity SQL (Trino / DuckDB)
â”‚ â”œâ”€â”€ sanity_checks_trino.sql
â”‚ â””â”€â”€ 90_joined_showcase/ # Portfolio-style analytical queries
â”‚
â”œâ”€â”€ legacy/ # Known-good historical scripts (immutable)
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ RESTORE_GUIDE.md
â”‚ â””â”€â”€ *.py
â”‚
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸ§  Design principles

- **Streaming only**  
  XML dumps are parsed incrementally. No full-file loads.

- **Typed-first schemas**  
  Canonical IDs are written as numeric types (`BIGINT`) where possible.
  Legacy string-based layouts are not used for new ingestion.

- **Schema stability > convenience**  
  Outputs are Trino/DuckDB friendly by design.
  No implicit type inference, no surprises.

- **Deterministic outputs**  
  Same input â†’ same Parquet layout â†’ same row counts.

- **Tests before trust**  
  Every pipeline has a runnable test producing:
  - row counts
  - null checks
  - referential sanity checks
  - small human-readable samples

---

## ğŸ— Output datasets

All pipelines write to a **data lake root**, typically defined by:

DISCOGS_DATA_LAKE=/absolute/path/to/discogs_data_lake/hive-data

graphql
Copy code

### Canonical physical datasets (typed)

$DISCOGS_DATA_LAKE/
â”œâ”€â”€ artists_v1_typed/
â”œâ”€â”€ artist_aliases_v1_typed/
â”œâ”€â”€ artist_memberships_v1_typed/
â”œâ”€â”€ masters_v1_typed/
â”œâ”€â”€ releases_v6/
â”œâ”€â”€ labels_v10/
â”œâ”€â”€ collection/
â””â”€â”€ warehouse_discogs/
â”œâ”€â”€ artist_name_map_v1/
â”œâ”€â”€ release_artists_v1/
â””â”€â”€ release_label_xref_v1/

kotlin
Copy code

These datasets are consumed by **Trino** as external tables and exposed via
logical views (`*_v1`) in the lakehouse layer.

### Test outputs

During tests, pipelines write to an isolated location:

$DISCOGS_DATA_LAKE/_tmp_test/

yaml
Copy code

Nothing touches production paths unless explicitly moved.

---

## Known upstream inconsistencies

Discogs data contains structural inconsistencies by design, including:

- artist aliases referencing missing artist IDs
- group memberships with partial metadata
- labels with parent references not resolvable in the same dump

These are upstream data characteristics, not pipeline errors.

Sanity tests are designed to:
- detect unexpected regressions
- quantify known anomalies
- prevent silent data corruption

---

## â–¶ï¸ Running a pipeline test

Example: **artists**

```bash
export DISCOGS_DATA_LAKE=/Users/you/discogs_data_lake/hive-data

./tests/run_test_artists_v1.sh \
  /Users/you/discogs_store/raw/artists/discogs_YYYYMMDD_artists.xml.gz
Each test script will:

run the pipeline into _tmp_test/

validate output using DuckDB

print row counts and sample rows

exit with PASS âœ… or fail hard

Same pattern applies to:

run_test_labels_v10.sh

run_test_masters_v1.sh

run_test_releases_v6.sh

Yes, releases is slow. Thatâ€™s reality, not a bug.

ğŸ” Sanity checks (Trino)
High-level integrity checks live in:

pgsql
Copy code
sql/sanity_checks_trino.sql
They validate:

primary key expectations

null ratios

referential integrity (artists â†” aliases, masters â†” releases)

known Discogs inconsistencies (documented, not hidden)

Run them after loading Parquet into Hive/Trino.

ğŸ§ª Why DuckDB + Trino
DuckDB

fast

local

ideal for pipeline validation and tests

Trino

distributed SQL engine

validates schemas at scale

runs heavy analytical and showcase queries

models realistic data-engineering workloads

ğŸ§“ Legacy directory (important)
legacy/ contains known-good historical scripts.

They are:

kept unchanged

documented

used as a reference baseline

If a refactor diverges, legacy scripts exist to prove what used to work.

ğŸš« What this repo is NOT
not a Discogs scraper

not a web app

not a demo toy

not shipping data

It is pipeline and validation infrastructure.

ğŸ‘¤ Author
Paolo Olivieri
Sound engineer â†’ data engineering pipelines
Focus: correctness, reproducibility, and real-world data pain

ğŸ“œ Notes
Discogs data is subject to Discogs licensing.
This repository focuses on pipelines, tests, and tooling, not redistribution.
